{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_vae.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandy1999/generateimage_vae/blob/master/mnist_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "tF-9wQo5_MDs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import libs \n",
        "import tensorflow as tf # for deep learning \n",
        "\n",
        "# helper libs \n",
        "import numpy as np # for matrix multiplication "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ENgU_2TU_MD5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "ae454697-5893-4f90-ef78-59b186c405a2"
      },
      "cell_type": "code",
      "source": [
        "# import datset \n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets('./data/MNIST/', one_hot=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-f4448fb1dd66>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./data/MNIST/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./data/MNIST/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting ./data/MNIST/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting ./data/MNIST/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VwV0Mrjy_MEm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# helper funtion for variables \n",
        "def weights_variable(shape):\n",
        "    inital = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(inital, name=\"W\")\n",
        "\n",
        "def biases_variable(shape):\n",
        "    inital = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(inital, name=\"B\")\n",
        "\n",
        "def FC_layer(X, W, B):\n",
        "    return tf.matmul(X,W) + B"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CyjHoRIO_MEr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f86ab1dd-131c-4ec2-cef6-bf7ce66d1a17"
      },
      "cell_type": "code",
      "source": [
        "# define placeholder for input as x \n",
        "pixels_dim = 28 * 28\n",
        "\n",
        "# input placeholder for image\n",
        "X = tf.placeholder(tf.float32, shape=([None, pixels_dim]))\n",
        "\n",
        "# a summary for the x_image \n",
        "tf.summary.image(\"Input\", tf.reshape(X,[-1, 28,28, 1]), 3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Input:0' shape=() dtype=string>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "R-tSeTBA_MEx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# encoder part \n",
        "latent_dim = 20\n",
        "h_dim = 500\n",
        "\n",
        "# layer 1\n",
        "with tf.name_scope(\"encoder\"):\n",
        "    w_enc = weights_variable([pixels_dim, h_dim])\n",
        "    b_enc = biases_variable([h_dim])\n",
        "    \n",
        "    h_enc = tf.nn.tanh(FC_layer(X, w_enc, b_enc))\n",
        "    \n",
        "    # write summary for the encoder \n",
        "    tf.summary.histogram(\"weights\", w_enc)\n",
        "    tf.summary.histogram(\"biases\", b_enc)\n",
        "    tf.summary.histogram(\"activation\", h_enc)\n",
        "    \n",
        "# layer 2 \n",
        "with tf.name_scope(\"mean\"):\n",
        "    w_mu = weights_variable([h_dim, latent_dim])\n",
        "    b_mu = biases_variable([latent_dim])\n",
        "    \n",
        "    mu = FC_layer(h_enc, w_mu, b_mu) # mean\n",
        "    \n",
        "    # write summary for the mean values \n",
        "    tf.summary.histogram(\"weights\", w_mu)\n",
        "    tf.summary.histogram(\"biases\",b_mu)\n",
        "    \n",
        "# layer 3 for std \n",
        "with tf.name_scope(\"std\"):\n",
        "    w_std = weights_variable([h_dim, latent_dim])\n",
        "    b_std = biases_variable([latent_dim])\n",
        "    \n",
        "    logstd = FC_layer(h_enc, w_std, b_std)\n",
        "    \n",
        "    # write summary for the model \n",
        "    tf.summary.histogram(\"weights\", w_std)\n",
        "    tf.summary.histogram(\"biases\", b_std)\n",
        "    \n",
        "# final output layer \n",
        "with tf.name_scope(\"enc_noise\"):\n",
        "    noise = tf.random_normal(shape=[1, latent_dim])\n",
        "    \n",
        "    z = mu + tf.multiply(noise, tf.exp(-5 * logstd))\n",
        "    \n",
        "    tf.summary.histogram(\"output\", z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CNru6Y7T_ME2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# decoder part \n",
        "\n",
        "with tf.name_scope(\"decoder\"):\n",
        "    w_dec = weights_variable([latent_dim,h_dim])\n",
        "    b_dec = biases_variable([h_dim])\n",
        "    \n",
        "    h_dec = tf.nn.tanh(FC_layer(z, w_dec, b_dec))\n",
        "    \n",
        "    # writing summary \n",
        "    tf.summary.histogram(\"weights\", w_dec)\n",
        "    tf.summary.histogram(\"biases\", b_dec)\n",
        "    tf.summary.histogram(\"activation\", h_dec)\n",
        "\n",
        "with tf.name_scope(\"reconstruct\"):\n",
        "    w_rec = weights_variable([h_dim, pixels_dim])\n",
        "    b_rec = biases_variable([pixels_dim])\n",
        "    \n",
        "    # activation using sigmoid \n",
        "    reconstruction = tf.nn.sigmoid(FC_layer(h_dec, w_rec, b_rec))\n",
        "    \n",
        "    # summary for reconstruct\n",
        "    tf.summary.histogram(\"weights\", w_rec)\n",
        "    tf.summary.histogram(\"biases\", b_rec)\n",
        "    tf.summary.histogram(\"activation\", reconstruction)\n",
        "    tf.summary.image(\"output\", tf.reshape(reconstruction,[-1, 28,28, 1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ljVabG19_ME6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# handelling loss functions \n",
        "with tf.name_scope(\"loss\"):\n",
        "    log_likelihood = tf.reduce_sum(X*tf.log(reconstruction + 1e-9)+(1 - X)*tf.log(1 - reconstruction + 1e-9), \n",
        "                                   reduction_indices=1)\n",
        "    \n",
        "    kl_term = -.5*tf.reduce_sum(1 + 2*logstd - tf.pow(mu,2) - tf.exp(2*logstd), reduction_indices=1)\n",
        "    \n",
        "    variation_lower_bound = tf.reduce_mean(log_likelihood - kl_term)\n",
        "    \n",
        "    # summary for these log terms \n",
        "    tf.summary.scalar(\"variational_lower_bound\", variation_lower_bound)\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.AdadeltaOptimizer(learning_rate=1e-4).minimize(-variation_lower_bound)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m5Qi6X-2_ME-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# training step \n",
        "# init all the variables and sessions \n",
        "sess = tf.Session()\n",
        "\n",
        "# init all the variables \n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# init a saver \n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-VwGCj8p_MFD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# write summaries \n",
        "summ = tf.summary.merge_all()\n",
        "\n",
        "# make a summary writer \n",
        "writer = tf.summary.FileWriter('./logs')\n",
        "# writing gaph for the session\n",
        "writer.add_graph(sess.graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "I4yjR_yQ_MFI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "6763daff-ca68-4b4f-f3fa-280629a835de"
      },
      "cell_type": "code",
      "source": [
        "num_iterations = 10000\n",
        "recording_interval = 1000\n",
        "#store value for these 3 terms so we can plot them later\n",
        "# variational_lower_bound_array = []\n",
        "# log_likelihood_array = []\n",
        "# KL_term_array = []\n",
        "# iteration_array = [i*recording_interval for i in range(num_iterations/recording_interval)]\n",
        "for i in range(num_iterations):\n",
        "    # np.round to make MNIST binary\n",
        "    #get first batch (200 digits)\n",
        "    x_batch = mnist.train.next_batch(200)[0]\n",
        "    #run our optimizer on our data\n",
        "    sess.run(optimizer, feed_dict={X: x_batch})\n",
        "    if (i%recording_interval == 0):\n",
        "        #every 1K iterations record these values\n",
        "        vlb_eval, s = sess.run([variation_lower_bound, summ], feed_dict={X:x_batch})\n",
        "        writer.add_summary(s, i)\n",
        "        print(\"Iteration: {}, Loss: {}\".format(i, vlb_eval))\n",
        "#         variational_lower_bound_array.append(vlb_eval)\n",
        "#         log_likelihood_array.append(np.mean(log_likelihood.eval(feed_dict={X: x_batch})))\n",
        "#         KL_term_array.append(np.mean(KL_term.eval(feed_dict={X: x_batch})))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 0, Loss: -859.2833862304688\n",
            "Iteration: 1000, Loss: -845.3290405273438\n",
            "Iteration: 2000, Loss: -856.7003173828125\n",
            "Iteration: 3000, Loss: -858.9381103515625\n",
            "Iteration: 4000, Loss: -855.9868774414062\n",
            "Iteration: 5000, Loss: -860.796142578125\n",
            "Iteration: 6000, Loss: -867.0031127929688\n",
            "Iteration: 7000, Loss: -851.03076171875\n",
            "Iteration: 8000, Loss: -840.2577514648438\n",
            "Iteration: 9000, Loss: -857.0206298828125\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}